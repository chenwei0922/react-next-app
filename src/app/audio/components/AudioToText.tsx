"use client";

import { Button, Card, Flex, Text } from "@radix-ui/themes";
import { useEffect, useRef, useState } from "react";

export const AudioToText = () => {

  const [texts, setTexts] = useState<string[]>([])
  const [interimTranscript, setInterimTranscript] = useState('');
  const [isListening, setIsListening] = useState(false);

  const recognitionRef = useRef<SpeechRecognition | null>(null);

  const [error, setError] = useState('')

  useEffect(() => {
    if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
      requestAnimationFrame(() => setError('æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«åŠŸèƒ½'))
      return;
    }
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    recognitionRef.current = new SpeechRecognition();
    const recognition = recognitionRef.current;

    recognition.continuous = true; // æ˜¯å¦è¿žç»­è¯†åˆ«
    recognition.interimResults = true; // æ˜¯å¦è¿”å›žä¸´æ—¶ç»“æžœ
    recognition.lang = 'zh-CN'; // è¯†åˆ«è¯­è¨€
    recognition.maxAlternatives = 3; // èŽ·å–å¤šä¸ªå¤‡é€‰ç»“æžœ

    recognition.onstart = () => {
      console.log('âœ… è¯­éŸ³è¯†åˆ«å¼€å§‹');
      setIsListening(true);
    };
    
    recognition.onresult = (event: SpeechRecognitionEvent) => {
      // console.log('ðŸŽ¯ è¯†åˆ«ç»“æžœ:', event);
 
      let finalTranscript = ''; // æœ€ç»ˆç»“æžœ
      let newInterimTranscript = ''; // ä¸´æ—¶ç»“æžœ

      for(let i = event.resultIndex; i < event.results.length; i++){
        const result = event.results[i]; // èŽ·å–è¯†åˆ«ç»“æžœ
        console.log('ðŸ” è¯†åˆ«ç»“æžœ:', result[0].transcript, result.isFinal)
        const alternative = result[0]; // èŽ·å–ç¬¬ä¸€ä¸ªå¤‡é€‰ç»“æžœ
        if(result.isFinal){
          // å¦‚æžœæ˜¯æœ€ç»ˆç»“æžœï¼Œåˆ™æ·»åŠ åˆ°æœ€ç»ˆç»“æžœä¸­
          finalTranscript += alternative.transcript;
        }else{
          // å¦‚æžœæ˜¯ä¸´æ—¶ç»“æžœï¼Œåˆ™æ·»åŠ åˆ°ä¸´æ—¶ç»“æžœä¸­
          newInterimTranscript += alternative.transcript;
        }
      }
      if(finalTranscript){
        setTexts(p=> [...p, finalTranscript])
      }
      setInterimTranscript(newInterimTranscript) // æ›´æ–°ä¸´æ—¶ç»“æžœ
    }

    recognition.onerror = (event: SpeechRecognitionErrorEvent) => {
      console.error('âŒ è¯†åˆ«é”™è¯¯:', event.error);
      setError(event.error)
      setIsListening(false);
    }
    recognition.onend = () => {
      console.log('â¹ï¸ è¯­éŸ³è¯†åˆ«ç»“æŸ');
      setIsListening(false);
    }
    return () => {
      recognitionRef.current?.stop();
    }
  }, [])

  const start = () => {
    const recognition = recognitionRef.current;
    if (recognition && !isListening) {
      try{
        recognition.start();
      }catch(error){
        console.error('å¯åŠ¨å¤±è´¥:', error);
      }
    }
  }
  const stop = () => {
    const recognition = recognitionRef.current;
    if (recognition && isListening) {
      recognition.stop();
    }
  }
  const clear = () => {
    setTexts([])
    setInterimTranscript('')
  }

  return (
    <Flex direction="column" gap="4">
      <Text className="text-center">Audio to Text</Text>
      <Card>
        <Text className="text-red-500">{error}</Text>
        <Flex direction={"column"} gap={"2"}>
          <Button onClick={start} disabled={isListening}>Start</Button>
          <Button onClick={stop} disabled={!isListening}>Stop</Button>
          <Button onClick={clear} >Clear</Button>
          <Text>æœ€ç»ˆçš„æ–‡æœ¬</Text>
          <Card>
            <Flex direction={'column'} className="max-h-[300px] overflow-auto">
              {texts.map((text, index) => {
                return (
                  <Text key={index}>{text}</Text>
                )
              })}
              <Text>{interimTranscript}</Text>
            </Flex>
          </Card>
        </Flex>
      </Card>
    </Flex>
  );
};
